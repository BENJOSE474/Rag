{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08325ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3055e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/RAGUDEMY/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54310986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x13df642f0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x31fdec350>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:gemma2-9b-it\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6643c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e387c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de89beb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of the complex question into smaller sub-questions:\n",
      "\n",
      "1. **What types of memory mechanisms does LangChain employ?** \n",
      "2. **How do LangChain agents leverage memory for task completion?**\n",
      "3. **What memory capabilities does CrewAI offer?**\n",
      "4. **How do CrewAI agents utilize memory in comparison to LangChain agents?** \n",
      "\n",
      "\n",
      "These sub-questions focus on specific aspects of memory and agents in both LangChain and CrewAI, allowing for more targeted and precise document retrieval. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4573a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f2af8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b079a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: Here are some sub-questions to break down the complex question:\n",
      "A: Please provide the complex question so I can help you break it down into sub-questions! ðŸ˜Š  \n",
      "\n",
      "I'm ready to analyze the context you provided and formulate relevant sub-questions. \n",
      "\n",
      "\n",
      "Q: **What types of memory mechanisms does LangChain utilize for its models?** (This focuses on LangChain's specific memory capabilities)\n",
      "A: According to the text, LangChain utilizes memory modules like **ConversationBufferMemory** and **ConversationSummaryMemory**. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about LangChain! \n",
      "\n",
      "\n",
      "Q: **How do LangChain agents leverage memory to perform tasks?** (This explores the application of memory within LangChain's agent framework)\n",
      "A: According to the provided context, LangChain agents use **context-aware memory** across steps. \n",
      "\n",
      "This means the agent can remember and utilize information from previous interactions and steps in a task to inform its current decisions and actions. \n",
      "\n",
      "\n",
      "Q: **What are the memory and agent capabilities offered by CrewAI?** (This directly addresses CrewAI's features in this domain)\n",
      "A: While the provided text describes CrewAI's core functionality as a multi-agent orchestration framework for building collaborative LLM-powered agents, it **doesn't explicitly mention specific memory or agent capabilities**. \n",
      "\n",
      "We know:\n",
      "\n",
      "* **Agents have defined roles:** Researcher, planner, executor, etc.\n",
      "* **Agents work semi-independently:**  They operate within a collaborative context.\n",
      "* **Agents communicate dynamically:** They share context and information with each other.\n",
      "\n",
      "However, details about the type of memory each agent possesses (e.g., short-term, long-term, external storage) or specific capabilities beyond their roles are not provided in the text. \n",
      "\n",
      "\n",
      "To answer your question directly, we need more information about CrewAI's architecture and how it handles memory and agent capabilities. \n",
      "\n",
      "\n",
      "\n",
      "Q: **What are the key differences in how LangChain and CrewAI handle memory and agent functionality?** (This prompts a comparative analysis of the two systems)\n",
      "A: Based on the provided context, here's a comparison of how LangChain and CrewAI handle memory and agent functionality:\n",
      "\n",
      "**LangChain:**\n",
      "\n",
      "* **Memory:** LangChain agents utilize \"context-aware memory\" which means they can remember information from previous steps in a conversation or task. This allows for more sophisticated interactions and decision-making.\n",
      "* **Agent Functionality:** LangChain agents operate on a \"planner-executor\" model.  \n",
      "\n",
      "    * **Planner:**  Analyzes the goal and devises a sequence of tool invocations needed to achieve it. This can involve dynamic decision-making, branching logic based on conditions, and incorporating memory from previous steps.\n",
      "    * **Executor:**  Actually carries out the planned sequence of tool calls.\n",
      "\n",
      "**CrewAI:**\n",
      "\n",
      "* **Memory:** The context doesn't explicitly detail CrewAI's memory management. \n",
      "* **Agent Functionality:** CrewAI focuses on \"role-based collaboration.\" This suggests it might be designed to manage interactions between different AI \"agents\" or personas, each with specific roles and responsibilities.  It likely handles coordination and communication between these roles.\n",
      "\n",
      "**Key Differences:**\n",
      "\n",
      "* **Focus:** LangChain emphasizes individual agent capabilities with its planner-executor model and context-aware memory, enabling complex task execution. CrewAI's focus is on managing collaboration between multiple AI agents, likely with predefined roles.\n",
      "* **Memory Details:** The context is unclear on CrewAI's memory mechanisms. It's possible CrewAI shares memory between its collaborating agents, but this isn't stated.\n",
      "* **Decision-Making:** LangChain's planner allows for dynamic decision-making and branching logic within a single agent. CrewAI's decision-making is likely more focused on coordinating actions between agents based on their defined roles.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any more questions!\n",
      "\n",
      "\n",
      "Q: These sub-questions target specific aspects of memory and agents in both LangChain and CrewAI, allowing for more focused and effective document retrieval\n",
      "A: The provided context highlights the **interoperability** between LangChain and CrewAI, but it doesn't delve into specific details about memory and agent functionalities within each system. \n",
      "\n",
      "To answer your sub-questions about memory and agents in LangChain and CrewAI, we need more information about:\n",
      "\n",
      "* **LangChain's memory mechanisms:** How does LangChain store and access past interactions within a conversation? Does it use a simple history buffer, external storage, or more sophisticated techniques like vector embeddings for semantic memory?\n",
      "* **CrewAI's role-based collaboration:** How does CrewAI manage different roles and their associated memories? Does each role have its own memory space, or is there a shared memory pool?\n",
      "* **Integration of memory and agents:** How do memory and agents interact in both LangChain and CrewAI? Does LangChain's memory inform agent actions, or vice versa?\n",
      "\n",
      "Once we have a clearer understanding of these aspects, we can provide more specific and insightful answers to your sub-questions. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
