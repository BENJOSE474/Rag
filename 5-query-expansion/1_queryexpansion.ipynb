{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7e855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decc311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step1 : Load and split the dataset\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498fadab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/RAGUDEMY/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x3090f3a70>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### step 2: Vector Store\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(chunks,embedding_model)\n",
    "\n",
    "## step 3:MMR Retriever\n",
    "retriever=vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":5})\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3fec94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x32147c6b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x328274d10>, root_client=<openai.OpenAI object at 0x3101931d0>, root_async_client=<openai.AsyncOpenAI object at 0x32147c9e0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 4 : LLM and Prompt\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(\"openai:o4-mini\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f83964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\\n\\nOriginal query: \"{query}\"\\n\\nExpanded query:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x32147c6b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x328274d10>, root_client=<openai.OpenAI object at 0x3101931d0>, root_async_client=<openai.AsyncOpenAI object at 0x32147c9e0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\n",
    "\n",
    "Original query: \"{query}\"\n",
    "\n",
    "Expanded query:\n",
    "\"\"\")\n",
    "\n",
    "query_expansion_chain=query_expansion_prompt| llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "289c18f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expanded query:\\n\\n“LangChain memory” OR “Lang Chain memory” OR “LangChain memory modules” OR “LangChain memory management” OR “LangChain conversation memory” OR “persistent memory in LangChain” OR “session state management” OR “buffer memory” OR “summary memory” OR “retrospective memory” OR “context retention” OR “stateful agent memory” OR “chat history storage” OR “memory retriever” OR “VectorStoreMemory” OR “RedisMemory” OR “InMemoryMemory” OR “SQLMemory” OR “embeddings cache” OR “vector embeddings” OR “RAG (retrieval-augmented generation)” OR “prompt context window” OR “memory plugin”'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\":\"Langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdd3da05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x3282a5fd0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x328277980>, root_client=<openai.OpenAI object at 0x3282a72c0>, root_async_client=<openai.AsyncOpenAI object at 0x3282a7350>, temperature=0.4, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define your prompt\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on context below.\n",
    "Context: {context}\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.4)\n",
    "\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dafbdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG answering prompt\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e52eaf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline with query expansion\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]}))\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02470a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:\n",
      "“LangChain memory support, memory types and modules in the LangChain Python framework; include short-term vs. long-term memory, ephemeral vs. persistent memory, ConversationBufferMemory, ConversationSummaryMemory, CombinedMemory, CompressedMemory, VectorStoreRetrieverMemory, RedisMemory, SQLMemory, file-based memory, token-window memory, timeline/multi-session memory, retrieval-augmented memory; context management, conversation state storage, chat history persistence, memory backends and strategies in LangChain.”\n",
      "✅ Answer:\n",
      " LangChain supports memory modules like ConversationBufferMemory and ConversationSummaryMemory.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1520131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:\n",
      "\n",
      "(\"CrewAI\" OR \"Crew AI\" OR \"Crew-AI\") AND (agent OR agents OR assistant OR assistants OR bot OR bots OR “autonomous agent” OR “autonomous agents” OR “intelligent agent” OR “intelligent agents” OR “virtual assistant” OR “virtual assistants” OR “digital agent” OR “digital agents” OR “multi-agent system” OR “agent-based model”) AND (crew management OR crew scheduling OR task allocation OR resource planning OR operations support OR “human-AI teaming” OR “decision support” OR “workload management” OR automation OR orchestration) AND (aviation OR maritime OR space OR “field service” OR “industrial operations” OR “logistics” OR “transportation”) AND (AI OR “artificial intelligence” OR machine learning OR “reinforcement learning” OR “deep learning” OR NLP OR “natural language processing” OR robotics OR “collaborative robotics”)\n",
      "✅ Answer:\n",
      " CrewAI agents are autonomous agents that have defined roles within a structured workflow, such as researcher, planner, or executor. They operate semi-independently within a collaborative context.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"CrewAI agents?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
