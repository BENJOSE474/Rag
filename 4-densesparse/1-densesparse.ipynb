{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e29ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b214e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x100782e40>, search_kwargs={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=[\n",
    "    Document(page_content=\"Langchain helps build LLM applications\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for sematic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in paris\"),\n",
    "     Document(page_content=\"France is a popular tourist destination.\"),\n",
    "      Document(page_content=\"The Eiffel Tower is a tower\"),\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6367a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x100782d80>, search_kwargs={})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dense Retriever(FAISS+HuggingFace)\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore=FAISS.from_documents(docs,embedding_model)\n",
    "dense_retriever=dense_vectorstore.as_retriever()\n",
    "dense_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c0a1e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x100782d80>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x12f9895e0>, k=3)], weights=[0.5, 0.5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sparse retriever(BM25)\n",
    "\n",
    "sparse_retriever=BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=3\n",
    "\n",
    "#comnibe with ensembler retriever\n",
    "hybrid_retriever=EnsembleRetriever(retrievers=[dense_retriever,sparse_retriever],\n",
    "weight=[0.2,0.8])\n",
    "hybrid_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "882d815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Document1:\n",
      "The Eiffel Tower is a tower\n",
      "\n",
      " Document2:\n",
      "Langchain helps build LLM applications\n",
      "\n",
      " Document3:\n",
      "The Eiffel Tower is located in paris\n",
      "\n",
      " Document4:\n",
      "France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "query=\"How can I build an Eiffel Tower ?\"\n",
    "results=hybrid_retriever.invoke(query)\n",
    "for i,doc in enumerate(results):\n",
    "    print(f\"\\n Document{i+1}:\\n{doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f38e7",
   "metadata": {},
   "source": [
    "RAG Pipeline with hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "710f9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "import os\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "060d7b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12f972ab0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x12f970c20>, root_client=<openai.OpenAI object at 0x12f9898e0>, root_async_client=<openai.AsyncOpenAI object at 0x12f989730>, temperature=0.4, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define your prompt\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on context below.\n",
    "Context: {context}\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.4)\n",
    "\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a7e8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=prompt)\n",
    "retriever_chain=create_retrieval_chain(retriever=hybrid_retriever,combine_docs_chain=document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d30b12b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " You can build an app using LLMs by using Langchain, which helps build LLM applications.\n",
      "\n",
      " Source Documents\n",
      "\n",
      " Doc1:Langchain helps build LLM applications\n",
      "\n",
      " Doc2:The Eiffel Tower is a tower\n",
      "\n",
      " Doc3:Pinecone is a vector database for sematic search.\n",
      "\n",
      " Doc4:France is a popular tourist destination.\n",
      "\n",
      " Doc5:The Eiffel Tower is located in paris\n"
     ]
    }
   ],
   "source": [
    "query={\"input\":\"How can I build an app using LLMs?\"}\n",
    "response=retriever_chain.invoke(query)\n",
    "\n",
    "print(\"Answer:\\n\",response[\"answer\"])\n",
    "print(\"\\n Source Documents\")\n",
    "for i,doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\n Doc{i+1}:{doc.page_content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4f948",
   "metadata": {},
   "source": [
    "RERANKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965f9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
