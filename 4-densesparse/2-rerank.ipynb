{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4865927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32328b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\\nDense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.\\nFAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\\nLangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=TextLoader(\"langchain_sample.txt\")\n",
    "raw_docs=loader.load()\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=400)\n",
    "docs=splitter.split_documents(raw_docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa7c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"How can i use lanchain to build an application with memory and tools?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee385cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x14d6d8a70>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "embedding_model=HuggingFaceBgeEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(docs,embedding_model)\n",
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\":8})\n",
    "retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e7b09b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x16016a9c0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x160212330>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=init_chat_model(\"groq:gemma2-9b-it\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53aaba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user's question.\n",
    "\n",
    "User Question: \"{question}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- Think about the relevance of each document to the user's question.\n",
    "- Return a list of document indices in ranked order, starting from the most relevant.\n",
    "\n",
    "Output format: comma-separated document indices (e.g., 2,1,3,0,...)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3448b3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fc43128d-5886-4a85-9b90-26718d0cb704', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='52a75c97-7523-46ef-b6a6-cc7afc5e8f10', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.'),\n",
       " Document(id='32c271d7-04e6-499a-91a0-1abb8e66cd61', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='ab373546-e0ee-4761-9ff5-f87aacda655b', metadata={'source': 'langchain_sample.txt'}, page_content='Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='4ccb228a-1c4b-4413-866e-2b72cfc73811', metadata={'source': 'langchain_sample.txt'}, page_content='Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='0953403c-5c08-4965-8306-1a037ed9adbc', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.'),\n",
       " Document(id='05ac1a5d-8a08-4b89-8b83-157dc6b7dd67', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='ce7bd8fb-1058-4ee6-85ea-fa08d27a3c04', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs=retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a537de66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user\\'s question.\\n\\nUser Question: \"{question}\"\\n\\nDocuments:\\n{documents}\\n\\nInstructions:\\n- Think about the relevance of each document to the user\\'s question.\\n- Return a list of document indices in ranked order, starting from the most relevant.\\n\\nOutput format: comma-separated document indices (e.g., 2,1,3,0,...)\\n')\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x16016a9c0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x160212330>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain= prompt| llm | StrOutputParser()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "915cb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lines = [f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "formatted_docs = \"\\n\".join(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7955cf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a ranking of the documents based on relevance to the user's question:\\n\\n**5, 1, 2, 4** \\n\\n**Explanation:**\\n\\n* **5 (Agents):** Directly addresses the user's interest in building applications with tools by explaining how LangChain agents utilize LLMs to choose and execute tools.\\n* **1 (Framework Overview):**  Provides a broad overview of LangChain, highlighting its core components including memory and tools, making it highly relevant to the user's question.\\n* **2 (Tool Integration):**  Focuses specifically on LangChain's tool integration capabilities, which is a key aspect of the user's request.\\n* **4 (Memory):** Addresses the user's need for memory in the application, explaining how LangChain achieves context retention.\\n\\n\\nLet me know if you have any other questions!\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"question\":query,\"documents\":formatted_docs})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8602680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Parse and rerank\n",
    "indices = [int(x.strip()) - 1 for x in response.split(\",\") if x.strip().isdigit()]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2c3cb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fc43128d-5886-4a85-9b90-26718d0cb704', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='52a75c97-7523-46ef-b6a6-cc7afc5e8f10', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs = [retrieved_docs[i] for i in indices if 0 <= i < len(retrieved_docs)]\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c14cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a43cc45",
   "metadata": {},
   "source": [
    "Implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d02874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d93a8aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load text file\n",
    "loader=TextLoader(\"langchain_sample.txt\")\n",
    "raw_docs=loader.load()\n",
    "raw_docs\n",
    "# Split text into document chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8900f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "## user query\n",
    "query=\"How can i use langchain to build an application with memory and tools?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87731cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/RAGUDEMY/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### FAISS and Huggingface model Embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(docs,embedding_model)\n",
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\":8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a7ded69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x151f67740>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3357d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenAI Embedding\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings()\n",
    "vectorstore_openai=FAISS.from_documents(docs,embeddings)\n",
    "retriever_openai=vectorstore_openai.as_retriever(search_kwargs={\"k\":8})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "884d5f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x31dc6f890>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x31d90aff0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## prompt and use the llm\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=init_chat_model(\"groq:gemma2-9b-it\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae753ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user's question.\n",
    "\n",
    "User Question: \"{question}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- Think about the relevance of each document to the user's question.\n",
    "- Return a list of document indices in ranked order, starting from the most relevant.\n",
    "\n",
    "Output format: comma-separated document indices (e.g., 2,1,3,0,...)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36c73060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='89a6013f-b05b-4df8-9fba-ed2a68003f49', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='5da93112-faca-4bd4-8e01-15e1035a2c6c', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='a603e7f0-867a-4e7d-9549-6e2de220d918', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='868e6b2d-de24-4772-a462-85476bca4157', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='f0fc7463-022c-448b-aefd-5a9bc9db3071', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(id='5b27708c-9111-404d-b8cd-7e4e1abd0d3d', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs=retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdd4357c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user\\'s question.\\n\\nUser Question: \"{question}\"\\n\\nDocuments:\\n{documents}\\n\\nInstructions:\\n- Think about the relevance of each document to the user\\'s question.\\n- Return a list of document indices in ranked order, starting from the most relevant.\\n\\nOutput format: comma-separated document indices (e.g., 2,1,3,0,...)\\n')\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x31dc6f890>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x31d90aff0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt| llm | StrOutputParser()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1039b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lines = [f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "formatted_docs = \"\\n\".join(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72275f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.',\n",
       " '2. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.',\n",
       " '3. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.',\n",
       " '4. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.',\n",
       " '5. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.',\n",
       " '6. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13bc3e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\\n2. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\\n3. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\\n4. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\\n5. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\\n6. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e12e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"2,1,4,5 \\n\\n\\nHere's why:\\n\\n* **Document 2** directly addresses the user's question by mentioning LangChain's capabilities for building applications with memory and tools. \\n* **Document 1**  highlights the memory and tool integration aspects of LangChain, making it highly relevant.\\n* **Document 4** focuses on agents, which are chains utilizing tools and memory, aligning with the user's interest.\\n* **Document 5**  discusses RAG, a technique often used in conjunction with tools and memory within LangChain applications.\\n\\n\\n\\n\\nLet me know if you have any other questions!\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"question\":query,\"documents\":formatted_docs})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88094248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [int(x.strip()) - 1 for x in response.split(\",\") if x.strip().isdigit()]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f92e8c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='5da93112-faca-4bd4-8e01-15e1035a2c6c', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='89a6013f-b05b-4df8-9fba-ed2a68003f49', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='868e6b2d-de24-4772-a462-85476bca4157', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs = [retrieved_docs[i] for i in indices if 0 <= i < len(retrieved_docs)]\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51f282dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Reranked Results:\n",
      "\n",
      "Rank 1:\n",
      "LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "\n",
      "Rank 2:\n",
      "LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "\n",
      "Rank 3:\n",
      "FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Show results\n",
    "print(\"\\nðŸ“Š Final Reranked Results:\")\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"\\nRank {i}:\\n{doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
